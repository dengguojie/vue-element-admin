#!/usr/bin/env python
# -*- coding: UTF-8 -*-
"""
Copyright 2019 Huawei Technologies Co., Ltd

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""
from op_test_frame.ut import OpUT
from impl.conv2d import *
from topi import generic

ut_case = OpUT("Conv2D", "impl.conv2d", "conv2d")

def test_bn(test_arg):
    import te
    from te import tvm
    from topi.cce import util
    import te.lang.cce
    from te.lang.cce import cce_build_code

    import sys
    from impl.bn_training_reduce import bn_training_reduce_compute
    from te.lang.cce import AutoScheduleOp
    sys.path.append("./llt/ops/ut/testcase_python")

    testcases = {
        "op_name": "conv_bn1",
        "all": {
            #pad_flag
            "conv_bn1_2_64_192_288_64_64_3_3_0_1_2":((2,64,192,288),(64,64,3,3),[[0,1],[0,1]],2),
            "conv_bn1_2_128_96_144_128_128_3_3_0_1_2":((2,128,96,144),(128,128,3,3),[[0,1],[0,1]],2),
            "conv_bn1_2_64_144_216_64_64_3_3_0_1_2":((2,64,144,216),(64,64,3,3),[[0,1],[0,1]],2),
            "conv_bn1_2_128_72_108_128_128_3_3_0_1_2":((2,128,72,108),(128,128,3,3),[[0,1],[0,1]],2),
            "conv_bn1_2_64_240_360_64_64_3_3_0_1_1":((2,64,240,360),(64,64,3,3),[[0,1],[0,1]],1),
            "conv_bn1_2_128_120_180_128_128_3_3_0_1_2":((2,128,120,180),(128,128,3,3),[[0,1],[0,1]],2),
            "conv_bn1_8_512_32_47_512_512_3_3_0_1":((8,512,32,47),(512,512,3,3),[[0,0],[0,0]],1),  #one-value no pass for bn1_sum, one-value no pass for conv_out

            # # "conv_perf_fp16_p001_2_16_786_1152_64_16_3_3_2_2_1_1_0_1_0_1":((2,16,768,1152),(64,16,3,3),[[0,1],[0,1]],2),
            "conv_bn1_2_64_384_576_64_64_3_3_1_1":((2,64,384,576),(64,64,3,3),[[1,1],[1,1]],1),
            "conv_bn1_2_64_384_576_128_64_3_3_1_1":((2,64,384,576),(128,64,3,3),[[1,1],[1,1]],1),
            "conv_bn1_2_128_192_288_256_128_1_1_0_1":((2,128,192,288),(256,128,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_128_192_288_64_128_1_1_0_1":((2,128,192,288),(64,128,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_64_192_288_64_64_3_3_1_1":((2,64,192,288),(64,64,3,3),[[1,1],[1,1]],1),
            "conv_bn1_2_64_192_288_256_64_1_1_0_1":((2,64,192,288),(256,64,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_256_192_288_64_256_1_1_0_1":((2,256,192,288),(64,256,1,1),[[0,0],[0,0]],1),
            # "conv_perf_fp16_p009_2_64_192_288_64_64_3_3_2_2_1_1_0_1_0_1":((2,64,192,288),(64,64,3,3),[[0,1],[0,1]],2),
            "conv_bn1_2_64_96_144_256_64_1_1_0_1":((2,64,96,144),(256,64,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_256_96_144_512_256_1_1_0_1":((2,256,96,144),(512,256,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_256_96_144_128_256_1_1_0_1":((2,256,96,144),(128,256,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_64_96_144_128_64_1_1_0_1":((2,64,96,144),(128,64,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_128_96_144_512_128_1_1_0_1":((2,128,96,144),(512,128,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_512_96_144_128_512_1_1_0_1":((2,512,96,144),(128,512,1,1),[[0,0],[0,0]],1),
            # "conv_perf_fp16_p016_2_128_96_144_128_128_3_3_2_2_1_1_0_1_0_1":((2,128,96,144),(128,128,3,3),[[0,1],[0,1]],2),
            "conv_bn1_2_128_48_72_512_128_1_1_0_1":((2,128,48,72),(512,128,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_512_48_72_1024_512_1_1_0_1":((2,512,48,72),(1024,512,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_512_48_72_256_512_1_1_0_1":((2,512,48,72),(256,512,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_256_48_72_256_256_3_3_1_1":((2,256,48,72),(256,256,3,3),[[1,1],[1,1]],1),
            "conv_bn1_2_256_48_72_1024_256_1_1_0_1":((2,256,48,72),(1024,256,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_1024_48_72_256_1024_1_1_0_1":((2,1024,48,72),(256,1024,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_1024_48_72_2048_1024_1_1_0_1":((2,1024,48,72),(2048,1024,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_1024_48_72_512_1024_1_1_0_1":((2,1024,48,72),(512,1024,1,1),[[0,0],[0,0]],1),
            # # "conv_perf_fp16_p025_8_512_26_38_512_512_3_3_1_1_1_1_0_0_0_0":((8,512,26,38),(512,512,3,3),[[0,0],[0,0]],1),
            "conv_bn1_2_512_48_72_2048_512_1_1_0_1":((2,512,48,72),(2048,512,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_2048_48_72_512_2048_1_1_0_1":((2,2048,48,72),(512,2048,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_2048_48_72_256_2048_1_1_0_1":((2,2048,48,72),(256,2048,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_1280_48_72_256_1280_1_1_0_1":((2,1280,48,72),(256,1280,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_256_48_72_256_256_1_1_0_1":((2,256,48,72),(256,256,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_2048_1_1_256_2048_1_1_0_1":((2,2048,1,1),(256,2048,1,1),[[0,0],[0,0]],1),

            # # "conv_perf_fp16_p032_2_16_576_864_64_16_3_3_2_2_1_1_0_1_0_1":((2,16,576,864),(64,16,3,3),[[0,1],[0,1]],2),
            "conv_bn1_2_64_288_432_64_64_3_3_1_1":((2,64,288,432),(64,64,3,3),[[1,1],[1,1]],1),
            "conv_bn1_2_64_288_432_128_64_3_3_1_1":((2,64,288,432),(128,64,3,3),[[1,1],[1,1]],1),
            "conv_bn1_2_128_144_216_256_128_1_1_0_1":((2,128,144,216),(256,128,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_128_144_216_64_128_1_1_0_1":((2,128,144,216),(64,128,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_64_144_216_64_64_3_3_1_1":((2,64,144,216),(64,64,3,3),[[1,1],[1,1]],1),
            "conv_bn1_2_64_144_216_256_64_1_1_0_1":((2,64,144,216),(256,64,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_256_144_216_64_256_1_1_0_1":((2,256,144,216),(64,256,1,1),[[0,0],[0,0]],1),
            # "conv_perf_fp16_p040_2_64_144_216_64_64_3_3_2_2_1_1_0_1_0_1":((2,64,144,216),(64,64,3,3),[[0,1],[0,1]],2),
            "conv_bn1_2_64_72_108_256_64_1_1_0_1":((2,64,72,108),(256,64,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_256_72_108_512_256_1_1_0_1":((2,256,72,108),(512,256,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_256_72_108_128_256_1_1_0_1":((2,256,72,108),(128,256,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_128_72_108_128_128_3_3_1_1":((2,128,72,108),(128,128,3,3),[[1,1],[1,1]],1),
            "conv_bn1_2_128_72_108_512_128_1_1_0_1":((2,128,72,108),(512,128,1,1),[[0,0],[0,0]],1),
            # # "conv_perf_fp16_p046_2_128_72_108_128_512_1_1_1_1_1_1_0_0_0_0":((2,128,72,108),(512,128,1,1),[[0,0],[0,0]],1),
            # "conv_perf_fp16_p047_2_128_72_108_128_128_3_3_2_2_1_1_0_1_0_1":((2,128,72,108),(128,128,3,3),[[0,1],[0,1]],2),


            "conv_bn1_2_128_36_54_512_128_1_1_0_1":((2,128,36,54),(512,128,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_512_36_54_1024_512_1_1_0_1":((2,512,36,54),(1024,512,1,1),[[0,0],[0,0]],1),  #one-value no pass,bn1_sum
            "conv_bn1_2_512_36_54_256_512_1_1_0_1":((2,512,36,54),(256,512,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_256_36_54_256_256_3_3_1_1":((2,256,36,54),(256,256,3,3),[[1,1],[1,1]],1),
            "conv_bn1_2_256_36_54_1024_256_1_1_0_1":((2,256,36,54),(1024,256,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_1024_36_54_256_1024_1_1_0_1":((2,1024,36,54),(256,1024,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_1024_36_54_2048_1024_1_1_0_1":((2,1024,36,54),(2048,1024,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_1024_36_54_512_1024_1_1_0_1":((2,1024,36,54),(512,1024,1,1),[[0,0],[0,0]],1),
            "conv_bn1_8_512_20_29_512_512_3_3_0_1":((8,512,20,29),(512,512,3,3),[[0,0],[0,0]],1),
            "conv_bn1_2_512_36_54_2048_512_1_1_0_1":((2,512,36,54),(2048,512,1,1),[[0,0],[0,0]],1),  #one-value no pass,bn1_sum
            "conv_bn1_2_2048_36_54_512_2048_1_1_0_1":((2,2048,36,54),(512,2048,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_2048_36_54_256_2048_1_1_0_1":((2,2048,36,54),(256,2048,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_1280_36_54_256_1280_1_1_0_1":((2,1280,36,54),(256,1280,1,1),[[0,0],[0,0]],1),
            # # "conv_perf_fp16_p061_2_256_36_54_3_256_1_1_1_1_1_1_0_0_0_0":((2,256,36,54),(3,256,1,1),[[0,0],[0,0]],1),
            # # "conv_perf_fp16_p062_2_16_959_1439_64_16_3_3_2_2_1_1_1_1_1_1":((2,16,959,1439),(64,16,3,3),[[1,1],[1,1]],2),



            "conv_bn1_2_64_480_720_64_64_3_3_1_1":((2,64,480,720),(64,64,3,3),[[1,1],[1,1]],1),
            "conv_bn1_2_64_480_720_128_64_3_3_1_1":((2,64,480,720),(128,64,3,3),[[1,1],[1,1]],1),
            "conv_bn1_2_128_240_360_256_128_1_1_0_1":((2,128,240,360),(256,128,1,1),[[0,0],[0,0]],1), #one-value no pass,bn1_sum
            "conv_bn1_2_128_240_360_64_128_1_1_0_1":((2,128,240,360),(64,128,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_64_240_360_64_64_3_3_1_1":((2,64,240,360),(64,64,3,3),[[1,1],[1,1]],1),
            "conv_bn1_2_64_240_360_256_64_1_1_0_1":((2,64,240,360),(256,64,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_256_240_360_64_256_1_1_0_1":((2,256,240,360),(64,256,1,1),[[0,0],[0,0]],1),
            # "conv_perf_fp16_p070_2_64_240_360_64_64_3_3_2_2_1_1_0_1_0_1":((2,64,240,360),(64,64,3,3),[[0,1],[0,1]],1),
            "conv_bn1_2_64_120_180_256_64_1_1_0_1":((2,64,120,180),(256,64,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_256_120_180_512_256_1_1_0_1":((2,256,120,180),(512,256,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_256_120_180_128_256_1_1_0_1":((2,256,120,180),(128,256,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_128_120_180_128_128_3_3_1_1":((2,128,120,180),(128,128,3,3),[[1,1],[1,1]],1),
            "conv_bn1_2_128_120_180_512_128_1_1_0_1":((2,128,120,180),(512,128,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_512_120_180_128_512_1_1_0_1":((2,512,120,180),(128,512,1,1),[[0,0],[0,0]],1),
            # "conv_perf_fp16_p077_2_128_120_180_128_128_3_3_2_2_1_1_0_1_0_1":((2,128,120,180),(128,128,3,3),[[0,1],[0,1]],2),
            "conv_bn1_2_128_60_90_512_128_1_1_0_1":((2,128,60,90),(512,128,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_512_60_90_1024_512_1_1_0_1":((2,512,60,90),(1024,512,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_512_60_90_256_512_1_1_0_1":((2,512,60,90),(256,512,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_256_60_90_256_256_3_3_1_1":((2,256,60,90),(256,256,3,3),[[1,1],[1,1]],1),
            "conv_bn1_2_256_60_90_1024_256_1_1_0_1":((2,256,60,90),(1024,256,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_1024_60_90_256_1024_1_1_0_1":((2,1024,60,90),(256,1024,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_1024_60_90_2048_1024_1_1_0_1":((2,1024,60,90),(2048,1024,1,1),[[0,0],[0,0]],1),  #one-value no pass,bn1_sum
            "conv_bn1_2_1024_60_90_512_1024_1_1_0_1":((2,1024,60,90),(512,1024,1,1),[[0,0],[0,0]],1),
            # "conv_perf_fp16_p086_8_512_32_47_512_512_3_3_1_1_1_1_0_0_0_0":((8,512,32,47),(512,512,3,3),[[0,0],[0,0]],1),
            "conv_bn1_2_512_60_90_2048_512_1_1_0_1":((2,512,60,90),(2048,512,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_2048_60_90_512_2048_1_1_0_1":((2,2048,60,90),(512,2048,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_2048_60_90_256_2048_1_1_0_1":((2,2048,60,90),(256,2048,1,1),[[0,0],[0,0]],1),
            "conv_bn1_2_1280_60_90_256_1280_1_1_0_1":((2,1280,60,90),(256,1280,1,1),[[0,0],[0,0]],1),
            # "conv_perf_fp16_p091_2_256_60_90_3_256_1_1_1_1_1_1_0_0_0_0":((2,256,60,90),(3,256,1,1),[[0,0],[0,0]],1), #why out_num < 16
        },
    }

    def conv_bn1_fusion(fm_shape, filter, pad, stride, key):
        from te.platform.cce_policy import disableL2
        disableL2()
        def fcombine(x, y):
            return x[0]+y[0], x[1]+y[1]

        def fidentity(t0, t1):
            return tvm.const(0, t0), tvm.const(0, t1)

        batch, channel, height, weight = fm_shape
        C0 = 16
        C1 = (channel + C0 -1)//C0
        shape_in = (batch, C1, height, weight, C0)

        out_channel = filter[0]
        in_channel_weight = filter[1]
        filter_h = filter[2]
        filter_w = filter[3]
        block_size_k = 16
        block_size_n = 16
        C_out = (out_channel + block_size_n - 1)//block_size_n
        shape_w = (in_channel_weight * filter_h * filter_w // block_size_k,
                                C_out, block_size_n, block_size_k)

        with tvm.target.cce():
            pad_h = pad[0]
            pad_w = pad[1]
            stride_h = stride
            stride_w = stride
            fm = tvm.placeholder(shape_in, name='fm', dtype='float16')
            filter_w = tvm.placeholder(shape_w, name='filter_w', dtype='float16')
            para_dict = {"pad_h": pad_h, "pad_w": pad_w, "stride_h": stride_h, "stride_w": stride_w, "filter_h": filter_h, "filter_w":filter_h, \
            "offset_x":0, "dilate_w":1, "dilate_h":1, "kernel_name": key}
            conv_res = te.lang.cce.conv(fm, filter_w, para_dict)
            conv_res_shape = conv_res.shape
            mean, variance = bn_training_reduce_compute(conv_res, {"format":"NC1HWC0"}, conv_res)
            conv_res_shape = tuple(conv_res.shape)
            reduce_shape = (conv_res_shape[1], conv_res_shape[3])
            k_0 = tvm.reduce_axis((0, conv_res_shape[0]), name='k_0')
            k_1 = tvm.reduce_axis((0, conv_res_shape[2]), name='k_1')
            cub = conv_res.op.input_tensors[0]
            c_col = cub.op.input_tensors[0]
            c_ub = tvm.compute(cub.shape,
                                lambda n, i, j, k: c_col(n, i, j, k),
                                name='c_ub',
                                tag="convolution_" + "c_ub",
                                attrs=cub.op.attrs)
            c_res = tvm.compute(conv_res_shape,
                                lambda *indice: c_ub(*indice),
                                name="C",
                                tag="convolution_" + "C")
            cast_0_ub = tvm.compute(conv_res_shape,
                                    lambda *indice:
                                    c_res(*indice).astype("float16"),
                                    name="cast_0_ub")
            cast_0 = tvm.compute(conv_res_shape,
                                    lambda *indice: cast_0_ub(*indice),
                                    name="cast_0")
            cast_1 = tvm.compute(conv_res_shape,
                                    lambda *indice: cast_0(*indice).astype("float32"),
                                    name="cast_1")
            mul_0 = te.lang.cce.vmul(cast_1, cast_1)

            tuple_reduce = tvm.comm_reducer(fcombine,
                                            fidentity,
                                            name='tuple_reduce')
            mean_out, _ = tvm.compute(reduce_shape,
                                        lambda c1, c0:
                                        tuple_reduce((cast_1[k_0, c1, k_1, c0],
                                                    mul_0[k_0, c1, k_1, c0]),
                                                    axis=[k_0, k_1]),
                                        name="mean_out")
            outputs = [cast_0, mean_out]
            auto_sch_res = AutoScheduleOp(outputs[1])

            sch = generic.auto_schedule([conv_res, mean, variance])

        config = {"print_ir": False,
                    "need_build": True,
                    "name": key,
                    "tensor_list": [fm, filter_w, sch.cce_special["real_out_tensor"][0],
                                                sch.cce_special["real_out_tensor"][1],
                                                sch.cce_special["real_out_tensor"][2]]}
        assert auto_sch_res.fusion_type == 20
        te.lang.cce.cce_build_code(sch, config)

    def run_testcase():
        testcases_for_all = testcases["all"]
        for key in testcases_for_all:
            try:
                conv_bn1_fusion(*testcases_for_all[key], key)
            except Exception as e:
                print("***run %s failed,error = %s" % (key, e))
    run_testcase()

print("adding Conv2D bn1 testcases")
ut_case.add_cust_test_func(test_func=test_bn)